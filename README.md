# Schema Mapper & Data Quality Fixer

![Python](https://img.shields.io/badge/Python-3.9%2B-blue)  
[![Streamlit](https://img.shields.io/badge/Streamlit-App-red)](https://schema-mapper-cleaner.streamlit.app/)
[Original High Clarity Video Link](https://drive.google.com/file/d/1RQxjc-pwdztZF8ejnZkuq0UNsLivWkCL/view?usp=sharing )

A production-ready **Streamlit application** that automatically maps messy CSV headers to a canonical schema and fixes data quality issues with intelligent suggestions and learning capabilities.

---

## ğŸš€ Features

- **Intelligent Header Mapping**: Automatically suggests mappings between input CSV headers and canonical schema fields using similarity algorithms
- **Deterministic Data Cleaning**: Automated cleaning and validation for phone numbers, emails, dates, currencies, postal codes, and more
- **Before/After Analytics**: Visual comparison of data quality metrics with interactive charts
- **Targeted Fix Suggestions**: Smart suggestions for remaining data quality issues (typos, invalid formats, missing codes)
- **Learning System**: Promotes and remembers successful fixes for future automatic application
- **Interactive UI**: Clean, intuitive Streamlit interface with step-by-step workflow
- **Production Ready**: Modular architecture, JSON persistence, error handling, and tests

---

## ğŸ“ Project Structure

```
project6_schema_mapper/
â”œâ”€â”€ app.py                  # Main Streamlit application
â”œâ”€â”€ mapping.py              # Header mapping logic with fuzzy matching
â”œâ”€â”€ cleaner.py              # Deterministic cleaning & validation functions
â”œâ”€â”€ fix_suggester.py        # Targeted fixes for remaining issues
â”œâ”€â”€ persistence.py          # JSON storage for promoted fixes
â”œâ”€â”€ schema_loader.py        # Canonical schema management
â”œâ”€â”€ utils.py                # Shared helper functions
â”œâ”€â”€ schema/
â”‚   â””â”€â”€ Project6StdFormat.csv    # Canonical schema definition
â”œâ”€â”€ sample_data/
â”‚   â”œâ”€â”€ Project6InputData1.csv   # Clean sample data
â”‚   â”œâ”€â”€ Project6InputData2.csv   # Messy headers and formats
â”‚   â””â”€â”€ Project6InputData3.csv   # Missing fields and nulls
â”œâ”€â”€ tests/                  # Unit tests
â”‚   â”œâ”€â”€ test_mapping.py
â”‚   â”œâ”€â”€ test_cleaner.py
â”‚   â””â”€â”€ test_fix_suggester.py
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ promoted_fixes.json     # Saved fixes (auto-created)
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Installation & Setup

1. **Clone the repository:**
```bash
git clone <repository-url>
cd project6_schema_mapper
```

2. **Create a virtual environment:**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies:**
```bash
pip install -r requirements.txt
```

4. **Run the application:**
```bash
streamlit run app.py
```

The app will be available at **http://localhost:8501**

---

## â˜ï¸ Deployment

To deploy on **Streamlit Cloud**:

1. Push your code and datasets to GitHub.  
2. Go to [Streamlit Cloud](https://share.streamlit.io/) and connect your repo.  
3. Set the entrypoint as `app.py`.  
4. Ensure `requirements.txt` is included.  

Thatâ€™s it â€” the app will be live on your Streamlit Cloud URL.

---

## ğŸ“Š Usage Workflow

### Step 1: Data Upload
- Upload your CSV file or select from sample datasets
- View basic file statistics and preview

### Step 2: Header Mapping
- Review auto-generated header mappings with confidence scores
- Override mappings manually if needed
- Confidence codes:
  - ğŸŸ¢ High (90%+)
  - ğŸŸ¡ Medium (70â€“89%)
  - ğŸ”´ Low (<70%)

### Step 3: Data Cleaning & Validation
- One-click cleaning and validation
- View before/after metrics and improvement charts
- Preview cleaned data sample

### Step 4: Targeted Fixes
- See remaining data quality issues
- Apply suggestions (email domains, phone codes, invalid dates, etc.)
- Promote successful fixes for future reuse

### Step 5: Final Report & Download
- Review final statistics & completeness
- Download cleaned CSV
- Visualize field-level quality metrics

---

## ğŸ§  System Design

### Deterministic First
The system **prioritizes deterministic transforms** (regex cleaning, parsing, normalization).  
AI-like techniques (fuzzy string matching, typo correction) are used **only for targeted fixes** when deterministic rules cannot fully resolve issues.

### Header Mapping Algorithm
1. **Exact Match** (Confidence: 1.0)  
2. **Promoted Aliases** (Confidence: 1.0)  
3. **Common Aliases** (Confidence: 0.95)  
4. **Token Overlap** (Confidence: 0.8â€“0.95)  
5. **Fuzzy String Matching** (Confidence: 0.6â€“0.75)  

### Cleaning Functions
- **Phone Numbers**: Normalize format, add country codes  
- **Emails**: Lowercase, fix domain typos  
- **Tax IDs/VAT**: Remove punctuation, uppercase  
- **Dates**: Parse and standardize to ISO format  
- **Currency/Numbers**: Remove symbols, convert to float  
- **Postal Codes**: Strip non-alphanumeric characters  
- **Websites**: Add protocols if missing  
- **Text Fields**: Trim whitespace, title case  
- **Missing Values**:
  - Numbers â†’ `0`  
  - Text â†’ `"Unknown"`  

### Fix Suggestions
- Email domain corrections (`gamil.com â†’ gmail.com`)  
- Missing phone country codes  
- Invalid date corrections  
- Malformed URL fixes  
- Postal code formatting  

### Learning & Memory
- Promoted fixes remembered in `promoted_fixes.json`  
- Aliases auto-applied in future uploads  

---

## ğŸ§ª Testing

Run tests with:
```bash
pytest tests/ -v
```

Covers:
- Header mapping logic  
- Cleaning functions  
- Fix suggestions  
- Persistence  

---

## ğŸ“‹ Sample Data

- **Project6InputData1.csv**: Clean dataset  
- **Project6InputData2.csv**: Messy headers & formats  
- **Project6InputData3.csv**: Missing fields & nulls  

---

## ğŸ“ˆ Performance

- Efficient for CSVs up to **10,000 rows**  
- Header mapping: **O(n Ã— m)** (headers Ã— schema fields)  
- Cleaning: **O(n)** (cells)  
- Memory usage: ~2Ã— CSV file size  

---

## ğŸ”® Future Enhancements

- ML-based data type detection  
- Cloud storage integration (S3, GCS)  
- API endpoints for programmatic access  
- Bulk processing  
- Custom validation rules  
- Advanced analytics dashboard  

---

For support or questions, please open an **issue** in this repository.  
